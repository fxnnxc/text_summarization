# Transformer-VAE Text Summarization



---

### Necessity(**N**)

* ğŸ¤ : only one or two sentences or simple idea(not main topic)
* ğŸ’› : must consider it 
* â¤ï¸ : highly or directly dependent  

---


## Models

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:--:|
[VAE-PGN based Abstractive Model in Multi-stage Architecture for Text Summarization](https://www.aclweb.org/anthology/W19-8664/)|-|ğŸ¤|[ğŸ“]()
[Toward Controlled Generation of Text](https://arxiv.org/abs/1703.00955)|-|-|-
[Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)|-|-|-






## Variational Approach

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:-:|
|[On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation(2019)](https://arxiv.org/abs/1909.13668)|-|ğŸ’›|[ğŸ“](https://github.com/fxnnxc/text_summarization/blob/main/study/variational/On-the-Importance-of-the-Kullback-Leibler-Divergence-Term-in-Variational-Autoencoders-for-Text-Generation.md)|
[Cyclical Annealing Schedule:A Simple Approach to Mitigating KL Vanishing](https://arxiv.org/abs/1903.10145)|||
[Semi-Amortized-Variational-Autoencoders](https://arxiv.org/abs/1802.02550)|||

## Inference

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:--:|



