# Transformer-VAE Text Summarization

---

### Necessity(**N**)

* ü§ç : only one or two sentences or simple idea(not main topic)
* üíõ : must consider it 
* ‚ù§Ô∏è : highly or directly dependent  

---
## Transformer based VAE
|Title|Keywords|Relevance|
|:-:|:-:|:--:|
|[A Transformer-Based Variational Autoencoder for Sentence Generation](https://arxiv.org/pdf/1909.09237.pdf)||

---

## Models

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:--:|
|[Attention Is All You Need](https://arxiv.org/abs/1706.03762)|||
|[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)|||
[VAE-PGN based Abstractive Model in Multi-stage Architecture for Text Summarization](https://www.aclweb.org/anthology/W19-8664/)|-|ü§ç|[üìù]()
[Toward Controlled Generation of Text](https://arxiv.org/abs/1703.00955)|-|-|-
[Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)|-|-|-
[Variational Neural Decoder for Abstractive Text Summarization](https://www.researchgate.net/publication/342445374_Variational_neural_decoder_for_abstractive_text_summarization)|||
[Transformer VAE: A Hierarchical Model for Structure-Aware and Interpretable Music Representation Learning](https://ieeexplore.ieee.org/document/9054554)|||
[Variational Autoencoders for Semi-supervised Text Classification](https://arxiv.org/abs/1603.02514)|||
[Generating Sentences from a Continuous Space](https://arxiv.org/abs/1511.06349)|||
[Latent Space Expanded Variational Autoencoder for Sentence Generation](https://ieeexplore.ieee.org/abstract/document/8853312)|||
[Improved Variational Autoencoders for Text Modeling using Dilated Convolutions](https://arxiv.org/abs/1702.08139)|||
[Neural Variational Inference for Text Processing](https://arxiv.org/abs/1511.06038)|||
[Guiding Generation for Abstractive Text Summarization Based on Key Information Guide Network](https://www.aclweb.org/anthology/N18-2009/)|||
[Self-Attention Guided Copy Mechanism for Abstractive Summarization](https://www.aclweb.org/anthology/2020.acl-main.125/)|||
[StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding](https://arxiv.org/abs/1908.04577)|||
[Sample Efficient Text Summarization Using a Single Pre-Trained Transformer](https://arxiv.org/abs/1905.08836)|||
[Learning by Semantic Similarity Makes Abstractive Summarization Better](https://arxiv.org/abs/2002.07767)|||
[Hierarchical Neural Story Generation](https://arxiv.org/abs/1805.04833)|||
[Deep Transformers with Latent Depth](https://arxiv.org/abs/2009.13102)|latent for selecting layer, Tranformer, VAE|4|‚ù§Ô∏è|
## Variational Approach

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:-:|
|[On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation(2019)](https://arxiv.org/abs/1909.13668)|-|üíõ|[üìù](https://github.com/fxnnxc/text_summarization/blob/main/study/variational/On-the-Importance-of-the-Kullback-Leibler-Divergence-Term-in-Variational-Autoencoders-for-Text-Generation.md)|
[Cyclical Annealing Schedule:A Simple Approach to Mitigating KL Vanishing](https://arxiv.org/abs/1903.10145)|||
[Semi-Amortized-Variational-Autoencoders](https://arxiv.org/abs/1802.02550)|||
[Isolating Sources of Disentanglement in Variational Autoencoders](https://arxiv.org/abs/1802.04942)|||
[Disentangling Disentanglement in Variational Autoencoders](https://arxiv.org/abs/1812.02833)|||
[InfoVAE: Information Maximizing Variational Autoencoders](https://arxiv.org/abs/1706.02262)|||
[VAE with a VampPrior](https://arxiv.org/abs/1705.07120)|||
[Iterative Amortized Inference](https://arxiv.org/abs/1807.09356)|||
[Understanding disentangling in Œ≤-VAE](https://arxiv.org/abs/1804.03599)|||
[Preventing Posterior Collapse with delta-VAEs](https://arxiv.org/abs/1901.03416)|||
[Lagging Inference Networks and Posterior Collapse in Variational Autoencoders](https://arxiv.org/abs/1901.05534)|||
[Spherical Latent Spaces for Stable Variational Autoencoders](https://arxiv.org/abs/1808.10805)|||

## Inference

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:--:|
[The Curious Case of Neural Text Degeneration](https://arxiv.org/abs/1904.09751)|sampling|||


