
<p align="center">
  <a href="https://github.com/pytorch/fairseq"><img src=https://img.shields.io/badge/fairseq-v0.10.1-blue?style=flat height=30px></a> 
  <img src=https://img.shields.io/badge/Python-v3.6.8-green?style=flat&logo=python height=30px></a> 
  <img src=https://img.shields.io/badge/Experiment-v4-orange?style=flat height=30px></a> 
</p>


# Text Summarization Using Latent Space in Transformer

A similar experiment with [experiment3](https://github.com/fxnnxc/text_summarization/tree/main/experiments/experiment3) but more stable code.


> Start Date : 2020.12.23

> Finish Date : ğŸ‘¨â€ğŸ’»

## Goal
Now using pretrained model is stable.  train vae-bart fully.


## Experiment

|code|Model|Beta scheduling|Loss|
|:-:|:-:|:-:|:-:|
|check_pretrained_model|BART|-|-|


## Source Codes


|name|info|
|:-:|:--|
|inference|script for inference|
|check_pretrained_model|script to check whether using pretrained model is same with defining new model and upload the parameters|

## Model info
|Index|Graph|info|
|---|---|---|
|-|-|-|



# ğŸ›©ï¸ Mini Experiments ğŸ›©ï¸

brief test and results
