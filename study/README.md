# Transformer-VAE Text Summarization



---

### Necessity(**N**)

* ü§ç : only one or two sentences or simple idea(not main topic)
* üíõ : must consider it 
* ‚ù§Ô∏è : highly or directly dependent  

---


## Models

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:--:|
|[BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://arxiv.org/abs/1910.13461)|||
[VAE-PGN based Abstractive Model in Multi-stage Architecture for Text Summarization](https://www.aclweb.org/anthology/W19-8664/)|-|ü§ç|[üìù]()
[Toward Controlled Generation of Text](https://arxiv.org/abs/1703.00955)|-|-|-
[Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/abs/1704.04368)|-|-|-






## Variational Approach

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:-:|
|[On the Importance of the Kullback-Leibler Divergence Term in Variational Autoencoders for Text Generation(2019)](https://arxiv.org/abs/1909.13668)|-|üíõ|[üìù](https://github.com/fxnnxc/text_summarization/blob/main/study/variational/On-the-Importance-of-the-Kullback-Leibler-Divergence-Term-in-Variational-Autoencoders-for-Text-Generation.md)|
[Cyclical Annealing Schedule:A Simple Approach to Mitigating KL Vanishing](https://arxiv.org/abs/1903.10145)|||
[Semi-Amortized-Variational-Autoencoders](https://arxiv.org/abs/1802.02550)|||

## Inference

|Title|Keywords|**N**|Analysis|
|---|:-:|:-:|:--:|



